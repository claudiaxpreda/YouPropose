{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XNbNUVEcvzAeMHn67RBlp_werdt2DSm-",
      "authorship_tag": "ABX9TyMKn6cOS2/EnUa9NI9tHL2X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiaxpreda/YouPropose/blob/main/data_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWtiJ44k8kWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c041716e-56ec-4289-d1f0-5bbb3802fb92"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import re\n",
        "import string                             \n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "\n",
        "\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from scipy.sparse import coo_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "\n",
        "WORKDIR = os.getcwd() + '/drive/MyDrive/SAC'\n",
        "US_PATH = WORKDIR + '/USvideos.csv'\n",
        "GB_PATH = WORKDIR + '/GBvideos.csv'\n",
        "CA_PATH = WORKDIR + '/CAvideos.csv'\n",
        "DE_PATH = WORKDIR + '/DEvideos.csv'\n",
        "FR_PATH = WORKDIR + '/FRvideos.csv'\n",
        "IN_PATH = WORKDIR + '/INvideos.csv'\n",
        "JP_PATH = WORKDIR + '/JPvideos.csv'\n",
        "KR_PATH = WORKDIR + '/KRvideos.csv'\n",
        "MX_PATH = WORKDIR + '/MXvideos.csv'\n",
        "RU_PATH = WORKDIR + '/RUvideos.csv'\n",
        "\n",
        "US_PATH_CAT = WORKDIR + '/US_category_id.json'\n",
        "GB_PATH_CAT = WORKDIR + '/GB_category_id.json'\n",
        "CA_PATH_CAT = WORKDIR + '/CA_category_id.json'\n",
        "DE_PATH_CAT = WORKDIR + '/DE_category_id.json'\n",
        "FR_PATH_CAT = WORKDIR + '/FR_category_id.json'\n",
        "IN_PATH_CAT = WORKDIR + '/IN_category_id.json'\n",
        "JP_PATH_CAT = WORKDIR + '/JP_category_id.json'\n",
        "KR_PATH_CAT = WORKDIR + '/KR_category_id.json'\n",
        "MX_PATH_CAT = WORKDIR + '/MX_category_id.json'\n",
        "RU_PATH_CAT = WORKDIR + '/RU_category_id.json'\n",
        "\n",
        "new_words = [\"youtube\",\"video\",\"channel\",\"link\", \n",
        "             \"also\", \"always\",\"one\", \"two\", \"three\",\n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\", \"ten\",\n",
        "            \"month\", \"today\", \"tomorrow\", \"comment\", \"like\", \"dislike\", \"tonight\", \"subscribe\", \"share\",\n",
        "            \"twitter\", \"snapchat\", \"instagram\", \"facebook\"]\n",
        "\n",
        "\n",
        "\n",
        "def get_path_country(code):\n",
        "  path = WORKDIR + '/{}videos.csv'.format(code)\n",
        "  path_cat = WORKDIR + '/{}_category_id.json'.format(code)\n",
        "\n",
        "  return (path, path_cat)\n",
        "\n",
        "def get_category_dict(CAT_PATH):\n",
        "  with open(CAT_PATH) as f:\n",
        "    data_json = json.load(f)[\"items\"]\n",
        "  \n",
        "  title_dict = {}\n",
        "  \n",
        "  for cat in data_json:\n",
        "    title_dict[int(cat[\"id\"])] = cat[\"snippet\"][\"title\"]\n",
        "\n",
        "  return title_dict\n",
        "\n",
        "def fill_null_values(index, value, data):\n",
        "  data[index] = data[index].fillna(value)\n",
        "\n",
        "\n",
        "def get_data(PATH, CAT_PATH):\n",
        "  data = pd.read_csv(PATH, encoding='utf-8')\n",
        "  data = data.drop(['comment_count', 'publish_time',\n",
        "              'thumbnail_link', 'comments_disabled',\n",
        "              'ratings_disabled'], 1)\n",
        "  \n",
        "  \n",
        "  title_dict = get_category_dict(CAT_PATH)\n",
        "  data[\"category_id\"] = data[\"category_id\"].map(title_dict)\n",
        "\n",
        "  fill_null_values('category_id', 'Other', data)\n",
        "  fill_null_values('description', 'NoDescription', data)\n",
        "\n",
        "  data = data[data.likes >= 1000]\n",
        "  data = data[data.views >= 1000]\n",
        "  \n",
        "\n",
        "  data['tags'] = np.where(data['tags'] == '[none]', data['title'], data['tags'])\n",
        "\n",
        "  data = data.drop_duplicates(['video_id'])\n",
        "  data = data.reset_index(drop=True)\n",
        "\n",
        "  return data\n",
        "  \n",
        "\n",
        "\n",
        "def clean_data(data):\n",
        "\n",
        "  html_tag=re.compile(r'<.*?>')\n",
        "  data=html_tag.sub(r'',data)\n",
        "\n",
        "  others_tag = re.compile(r'&lt;/?.*?&gt;\",\" &lt;&gt; ')\n",
        "  data = others_tag.sub(r'', data)\n",
        "\n",
        "  hash_tag = re.compile(r'/#\\w+\\s*/')\n",
        "  data = hash_tag.sub(r'', data)\n",
        "\n",
        "  integer_tag = re.compile(r'$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$')\n",
        "  data = integer_tag.sub(r'', data)\n",
        "\n",
        "  links_tag = 'http\\S+'\n",
        "  data = re.sub(links_tag, '',data , flags=re.MULTILINE)\n",
        "\n",
        "  spaces_clean= re.compile(r'\\s\\s+')\n",
        "  data=spaces_clean.sub(r'',data)\n",
        "\n",
        "  emoji_clean= re.compile(\"[\"\n",
        "                          u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                          u\"\\U00002702-\\U000027B0\"\n",
        "                          u\"\\U000024C2-\\U0001F251\"\n",
        "                          \"]+\", flags=re.UNICODE)\n",
        "  data=emoji_clean.sub(r'',data)\n",
        "\n",
        "  data = data.replace('\"', \"\")\n",
        "\n",
        "\n",
        "  return data\n",
        "\n",
        "def apply_clean_data(data, tag, oldtag):\n",
        "  data[tag] = data[oldtag].apply(lambda z: clean_data(z))\n",
        "  data[tag] = data[tag].str.lower()\n",
        "\n",
        "   \n",
        "def lemm_data(comment):\n",
        "  lemm = WordNetLemmatizer()\n",
        "  lemm_words =[lemm.lemmatize(word).lower() for word in comment] \n",
        "  return lemm_words\n",
        "\n",
        "def apply_lemm_data(data, tag):\n",
        "  data[tag]=data[tag].apply(lambda z: lemm_data(z))\n",
        "\n",
        "def lemm_description(data, tag):\n",
        "  lemm = WordNetLemmatizer()\n",
        "  data[tag] = data[tag].apply(\n",
        "      lambda z : ' '.join([lemm.lemmatize(word).lower() for word in z.split()])) \n",
        "\n",
        "\n",
        "def split_by_mark(data, tag, mark):\n",
        "  data[tag] = data[tag].astype(str)\n",
        "  data[tag] = data[tag].str.split(mark)\n",
        "  \n",
        "def clean_tags(data):\n",
        "  apply_clean_data(data, \"clean_t\", 'tags')\n",
        "  split_by_mark(data, \"clean_t\", '|')\n",
        "  apply_lemm_data(data, \"clean_t\")\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "  text = re.sub(\"https*\\S+\", \" \", text)\n",
        "  text = re.sub(\"@\\S+\", \" \", text)\n",
        "  text = re.sub(\"#\\S+\", \" \", text)\n",
        "  text = re.sub(\"\\'\\w+\", \" \", text)\n",
        "  text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "  text = re.sub(r'\\w*\\d+\\w*', '', text)\n",
        "  text = re.sub('\\s{2,}', \" \", text)\n",
        "  emoji_clean = re.compile(\"[\"\n",
        "                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                    u\"\\U00002702-\\U000027B0\"\n",
        "                    u\"\\U000024C2-\\U0001F251\"\n",
        "                    \"]+\", flags=re.UNICODE)\n",
        "  text = emoji_clean.sub(r'', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "def apply_clean_text(data, tag):\n",
        "  data[tag] = data[tag].apply(lambda z: clean_text(z))\n",
        "\n",
        "def clean_description(data):\n",
        "  data['clean_d'] = data['description'].apply(\n",
        "      lambda z: z.replace(\"-\", \" \"))\n",
        "  # apply_clean_data(data, 'description')\n",
        "  data['clean_d'] = data['clean_d'].apply(\n",
        "      lambda z: z.replace(\"\\\\n\\\\n\", \"\\n\"))\n",
        "  data['clean_d'] = data['clean_d'].apply(\n",
        "      lambda z: z.replace(\"\\\\n\", \"\\n\"))\n",
        "  apply_clean_text(data, 'clean_d')\n",
        "  lemm_description(data, 'clean_d')\n",
        "\n",
        "def clean_title():\n",
        "  pass\n",
        "\n",
        "def get_stop_list(language):\n",
        "  stoplist = list(string.punctuation)\n",
        "  stoplist += stopwords.words(language)\n",
        "  stoplist += new_words\n",
        "  return stoplist\n",
        "\n",
        "\n",
        "def get_tf_idf_keywords(stoplist, data):\n",
        "  tf_idf = TfidfVectorizer(max_df=0.8,stop_words=stoplist, max_features=10000, ngram_range=(1,2))\n",
        "  tf_idf.fit(data.clean_d)\n",
        "  return tf_idf\n",
        "\n",
        "def get_keywords(entry, tf_idf):\n",
        "  doc = pd.Series(entry)\n",
        "  doc_vector = tf_idf.transform(doc)\n",
        "  sorted_items=sort_coo(doc_vector.tocoo())\n",
        "  feature_names = tf_idf.get_feature_names()\n",
        "  keywords=extract_topn_from_vector(feature_names,sorted_items,15)\n",
        "  keywords = list(keywords.keys())\n",
        "  return keywords\n",
        "\n",
        "def apply_get_keywords(data, language):\n",
        "  stoplist = get_stop_list(language)\n",
        "  tf_idf = get_tf_idf_keywords(stoplist, data)\n",
        "  data['keywords'] = data['clean_d'].apply(lambda z : get_keywords(z,tf_idf))\n",
        "  \n",
        "\n",
        "# apply_get_keywords(us_data)\n",
        "\n",
        "\n",
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        " \n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def all_words(data):\n",
        "  data['all'] = data['keywords'] + data['clean_t']\n",
        "\n",
        "\n",
        "def join_words(data):\n",
        "  new_list = []\n",
        "  for w in  data:\n",
        "    if len(w) > 1:\n",
        "      new_list.append(\"\".join(w.split()))\n",
        "    else:\n",
        "      new_list.append(w)\n",
        "  \n",
        "  return new_list\n",
        "\n",
        "def apply_join(data):\n",
        "  all_words(data)\n",
        "  data['all'] = data['all'].apply(lambda z: join_words(z))\n",
        "  data['all'] = data['all'].apply(lambda z: \" \".join(z))\n",
        "  data['channel_title'] = data['channel_title'].str.lower()\n",
        "  data[\"channel_title\"].apply(lambda z: \" \".join(z))\n",
        "  data['all'] = data['all'] + \" \" + data['category_id'].str.lower() + \" \" + data['channel_title']\n",
        "\n",
        "\n",
        "def get_clean_data_final(code, language):\n",
        "  path, path_cat = get_path_country(code)\n",
        "  data =  get_data(path, path_cat)\n",
        "  print(data.size)\n",
        "  clean_description(data)\n",
        "  clean_tags(data)\n",
        "  apply_get_keywords(data, language)\n",
        "  apply_join(data)\n",
        "  return data\n",
        "\n",
        "def save_clean_data(code, language, path):\n",
        "  data = get_clean_data_final(code, language)\n",
        "  # data.to_json('/content/drive/MyDrive/SAC/US_final.json')\n",
        "  data.to_json(path)\n",
        "\n",
        "def clean_entry_data(path, cat_path):\n",
        "  #data_e = pd.read_json(path)\n",
        "  with open(\"/content/drive/MyDrive/SAC/SAC_new_data.json\") as f:\n",
        "    data_json = json.load(f)['videos']\n",
        "  \n",
        "  data = pd.DataFrame(data_json)\n",
        "\n",
        "  title_dict = get_category_dict(cat_path)\n",
        "  \n",
        "  data[\"category_id\"] = data[\"categoryId\"].map(title_dict)\n",
        "\n",
        "  fill_null_values('category_id', 'Other', data)\n",
        "  fill_null_values('description', 'NoDescription', data)\n",
        "\n",
        "  data['tags'] = data['tags'].apply(lambda z : \"|\".join(z))\n",
        "  data['tags'] = np.where(data['tags'] == '[none]', data['title'], data['tags'])\n",
        "  data['channel_title'] = data['channelName']\n",
        "  data['video_id'] = data['id']\n",
        "\n",
        "\n",
        "  print(data.size)\n",
        "  clean_description(data)\n",
        "  clean_tags(data)\n",
        "  apply_get_keywords(data, 'english')\n",
        "  apply_join(data)\n",
        "\n",
        "  return data\n",
        "\n",
        "def test():\n",
        "  data = get_clean_data_final('US', 'english')\n",
        "  data.head()\n",
        "  return data "
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aafw6uw9Qet5",
        "outputId": "28b49e31-7f65-47e4-e888-befb23357164"
      },
      "source": [
        "dat = test()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyjk1kqlUlr_"
      },
      "source": [
        "dat['all'].iloc[1]\n",
        "dat.to_json('/content/drive/MyDrive/SAC/US-final-2.json')"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3Pr3JSFYgA2"
      },
      "source": [
        "import pandas as pd\n",
        "data_1 = pd.read_json('/content/drive/MyDrive/SAC/US-final-2.json')\n",
        "data_1 = data_1.drop_duplicates(['video_id'])\n",
        "data_1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dx9JxSaLCCi"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def get_tf_idf_cosine(data, stopwords):\n",
        "  tfv = TfidfVectorizer(\n",
        "      max_features = None,\n",
        "      strip_accents = \"unicode\",\n",
        "      analyzer = \"word\",\n",
        "      token_pattern = r'\\w{1,}',\n",
        "      ngram_range = (1, 2),         # Taking combinations of 1-3 different kind of words\n",
        "      stop_words = stopwords        # Remove the unnecessary stopword characters\n",
        "  )\n",
        "  tfv_matrix = tfv.fit_transform(data['all'])   # => Sparse Matrix(vectors) => most of the values in matrix = 0\n",
        "  return tfv_matrix\n",
        "\n",
        "def get_matrix(tfv_matrix):\n",
        "  cosine_sim = cosine_similarity(tfv_matrix, tfv_matrix)\n",
        "  return cosine_sim\n",
        "\n",
        "def recommendations(video_id, data, cosine_sim, indices):\n",
        "    recommended_videos = []\n",
        "    idx = indices[video_id]\n",
        "    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending = False)\n",
        "    top_10_indexes = list(score_series.iloc[1:6].index)\n",
        "    for i in top_10_indexes:\n",
        "        # recommended_movies.append(list(df_cleaned.title)[i] + \"--- Labels ---- \" + list(df_cleaned.video_id)[i])\n",
        "        recommended_videos.append(list(data.video_id)[i])\n",
        "        \n",
        "    return recommended_videos\n",
        "\n",
        "def get_indices(data):\n",
        "  df_cleaned = data[['video_id', 'title', 'all', 'category_id']]\n",
        "  df_cleaned = df_cleaned.drop_duplicates(['video_id'])\n",
        "  df_cleaned = df_cleaned.reset_index(drop=True)\n",
        "  indices = pd.Series(df_cleaned.index, index=df_cleaned['video_id'])\n",
        "  return df_cleaned, indices\n",
        "\n",
        "def post():\n",
        "  pass\n",
        "\n",
        "def get_api():\n",
        "  path = '/content/drive/MyDrive/SAC/US_category_id.json'\n",
        "\n",
        "  data  = clean_entry_data('', path)\n",
        "  data_1 = pd.read_json('/content/drive/MyDrive/SAC/CA-final.json')\n",
        "  len =  data_1.size if data_1.size < 30000 else 30000\n",
        "  datac = pd.concat([data, data_1[:len]])\n",
        "  new_data, indices = get_indices(datac)\n",
        "  tfv_matrix = get_tf_idf_cosine(new_data, get_stop_list('english'))\n",
        "  cosine_sim= get_matrix(tfv_matrix)\n",
        "\n",
        "  result = []\n",
        "  for id in data['video_id']:\n",
        "    result = result + recommendations(id, new_data, cosine_sim, indices)\n",
        "\n",
        "  return result\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4glPwz-8ZdeB",
        "outputId": "7506446d-2650-4321-a467-e0fe664e543b"
      },
      "source": [
        "print(get_api())\n",
        "\n",
        "result = get_api()\n",
        "mylist = list( dict.fromkeys(result) )\n",
        "print(mylist) "
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n",
            "['lp-EO5I60KA', '2Vv-BfVoq4g', 'iWZmdoY1aTE', 'UDDMYw_IZnE', '817P8W8-mGE', 'PNCmrJI743E', 'D-nqqawsZfI', 'OMoY0SoP_b0', 'ZKchfFiHje0', '9pqaQvfHK-k', 'rsEne1ZiQrk', 'M4ZoCHID9GI', 'xnDHnm4jcMc', 'XR7Ev14vUh8', 'jnQ4V-wajLY', 'DDbx1uArVOM', 'bS9zXmexXUQ', 'h0zMjd5ZAJ4', 'aLRQLkqjHEg', 'KCCbbvAKxoc', 'JGwWNGJdvx8', '2Vv-BfVoq4g', 'iWZmdoY1aTE', '817P8W8-mGE', 'UDDMYw_IZnE']\n",
            "60\n",
            "['lp-EO5I60KA', '2Vv-BfVoq4g', 'iWZmdoY1aTE', 'UDDMYw_IZnE', '817P8W8-mGE', 'PNCmrJI743E', 'D-nqqawsZfI', 'OMoY0SoP_b0', 'ZKchfFiHje0', '9pqaQvfHK-k', 'rsEne1ZiQrk', 'M4ZoCHID9GI', 'xnDHnm4jcMc', 'XR7Ev14vUh8', 'jnQ4V-wajLY', 'DDbx1uArVOM', 'bS9zXmexXUQ', 'h0zMjd5ZAJ4', 'aLRQLkqjHEg', 'KCCbbvAKxoc', 'JGwWNGJdvx8']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuYoyI8J9umw",
        "outputId": "1c4e533d-5817-4b50-ae2b-dc839fd47b8d"
      },
      "source": [
        "print(get_api())\n",
        "result = get_api()"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60\n",
            "                                               title  ... video_error_or_removed\n",
            "0   Ed Sheeran - Shape of You (Official Music Video)  ...                    NaN\n",
            "1  Biden: Health care orders undo the damage Trum...  ...                    NaN\n",
            "2            The Weeknd - The Hills (Official Video)  ...                    NaN\n",
            "3  Joe Biden 2021 Presidential Inauguration Ceremony  ...                    NaN\n",
            "4  Ed Sheeran - Thinking Out Loud (Official Music...  ...                    NaN\n",
            "\n",
            "[5 rows x 21 columns]\n",
            "['lp-EO5I60KA', 'ZwvbQR887W0', 'yzTuBuRdAyA', 'I-QOOx_K9V0', 'ZwvbQR887W0', 'lp-EO5I60KA', 'yzTuBuRdAyA', 'JGwWNGJdvx8', 'lp-EO5I60KA', 'ZwvbQR887W0', 'I-QOOx_K9V0', 'JGwWNGJdvx8', 'I-QOOx_K9V0', 'lp-EO5I60KA', 'yzTuBuRdAyA', 'JGwWNGJdvx8', 'JGwWNGJdvx8', 'ZwvbQR887W0', 'yzTuBuRdAyA', 'I-QOOx_K9V0']\n",
            "60\n",
            "                                               title  ... video_error_or_removed\n",
            "0   Ed Sheeran - Shape of You (Official Music Video)  ...                    NaN\n",
            "1  Biden: Health care orders undo the damage Trum...  ...                    NaN\n",
            "2            The Weeknd - The Hills (Official Video)  ...                    NaN\n",
            "3  Joe Biden 2021 Presidential Inauguration Ceremony  ...                    NaN\n",
            "4  Ed Sheeran - Thinking Out Loud (Official Music...  ...                    NaN\n",
            "\n",
            "[5 rows x 21 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tob_gUkb1jx",
        "outputId": "9a281cc9-609d-4093-8b86-2f212614c6bc"
      },
      "source": [
        "for i in mylist:\n",
        "  print(datac[datac['video_id'] == i][['title','video_id']])"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               title     video_id\n",
            "4  Ed Sheeran - Thinking Out Loud (Official Music...  lp-EO5I60KA\n",
            "                                               title     video_id\n",
            "3  Joe Biden 2021 Presidential Inauguration Ceremony  ZwvbQR887W0\n",
            "                                     title     video_id\n",
            "2  The Weeknd - The Hills (Official Video)  yzTuBuRdAyA\n",
            "                                               title     video_id\n",
            "1  Biden: Health care orders undo the damage Trum...  I-QOOx_K9V0\n",
            "                                              title     video_id\n",
            "0  Ed Sheeran - Shape of You (Official Music Video)  JGwWNGJdvx8\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}